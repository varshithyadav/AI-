import numpy as np

class DecisionTreeClassifier:
    def __init__(self):
        self.tree = {}

    def train(self, X, y):
        self.tree = self._build_tree(X, y)

    def _build_tree(self, X, y):
        n_samples, n_features = X.shape
        unique_classes = np.unique(y)

        # If all labels are the same, return leaf node
        if len(unique_classes) == 1:
            return {'class': unique_classes[0]}

        # If there are no features left, return leaf node with majority class
        if n_features == 0:
            return {'class': np.argmax(np.bincount(y))}

        best_feature, best_threshold = self._find_best_split(X, y)

        # If no best split found, return leaf node with majority class
        if best_feature is None:
            return {'class': np.argmax(np.bincount(y))}

        # Split data based on the best feature and threshold
        left_indices = X[:, best_feature] < best_threshold
        right_indices = ~left_indices

        left_subtree = self._build_tree(X[left_indices], y[left_indices])
        right_subtree = self._build_tree(X[right_indices], y[right_indices])

        return {'feature': best_feature, 'threshold': best_threshold,
                'left': left_subtree, 'right': right_subtree}

    def _find_best_split(self, X, y):
        best_gini = float('inf')
        best_feature = None
        best_threshold = None

        for feature in range(X.shape[1]):
            thresholds = sorted(set(X[:, feature]))

            for threshold in thresholds:
                left_indices = X[:, feature] < threshold
                right_indices = ~left_indices

                gini = self._calculate_gini(y[left_indices], y[right_indices])

                if gini < best_gini:
                    best_gini = gini
                    best_feature = feature
                    best_threshold = threshold

        return best_feature, best_threshold

    def _calculate_gini(self, left_labels, right_labels):
        n_total = len(left_labels) + len(right_labels)
        gini_left = self._gini_impurity(left_labels)
        gini_right = self._gini_impurity(right_labels)

        return (len(left_labels) / n_total) * gini_left + (len(right_labels) / n_total) * gini_right

    def _gini_impurity(self, labels):
        if len(labels) == 0:
            return 0

        unique_classes, class_counts = np.unique(labels, return_counts=True)
        p = class_counts / len(labels)
        gini = 1 - np.sum(p ** 2)

        return gini

    def predict(self, X):
        return [self._predict_sample(sample) for sample in X]

    def _predict_sample(self, sample):
        node = self.tree

        while 'class' not in node:
            if sample[node['feature']] < node['threshold']:
                node = node['left']
            else:
                node = node['right']

        return node['class']

# Sample data
X = np.array([[2, 3],
              [5, 4],
              [9, 6],
              [4, 7],
              [8, 1]])
y = np.array([0, 1, 1, 0, 1])

# Initialize and train the classifier
clf = DecisionTreeClassifier()
clf.train(X, y)

# Make predictions
predictions = clf.predict(X)
print("Predictions:", predictions)
